{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinqian/.pyenv/versions/3.12.7/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# load image from the IAM database (actually this model is meant to be used on printed text)\n",
    "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Jupyter Notebook Conversion\n",
    "# Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels=3), transforms.ToTensor()]\n",
    ")\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"../../data/\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "def visualize_sample(dataset, index):\n",
    "    \"\"\"Visualize a single sample from the dataset.\"\"\"\n",
    "    image, label = dataset[index]\n",
    "    plt.imshow(image[0], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize a few samples\n",
    "for i in range(3):\n",
    "    visualize_sample(train_data, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for Hugging Face's Dataset API\n",
    "dataset_dict = {\n",
    "    \"image\": [train_data[i][0].numpy() for i in range(len(train_data))],\n",
    "    \"label\": [int(train_data[i][1]) for i in range(len(train_data))],\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initialize the Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(\"mps\")\n",
    "\n",
    "# Adjust model config for MNIST\n",
    "model.config.decoder.vocab_size = 11  # 10 digits + <eos> token\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.max_length = 5  # Ensure the decoder output has a fixed sequence length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "# Step 3: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trained_model/\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  46%|████▌     | 46/100 [00:00<00:00, 222.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  69%|██████▉   | 69/100 [00:00<00:00, 223.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  94%|█████████▍| 94/100 [00:00<00:00, 228.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 105.81 examples/s]\n",
      "Map:  22%|██▏       | 22/100 [00:00<00:00, 214.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  72%|███████▏  | 72/100 [00:00<00:00, 234.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 7\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 1\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 9\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 5\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 0\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 3\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 6\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  98%|█████████▊| 98/100 [00:00<00:00, 239.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 2\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 8\n",
      "Pixel values (type): <class 'torch.Tensor'>\n",
      "Pixel values (shape): torch.Size([3, 384, 384])\n",
      "Label (type): <class 'torch.Tensor'>\n",
      "Label (value): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 114.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def preprocess_data(example):\n",
    "    pixel_values = processor(\n",
    "        images=torch.tensor(example[\"image\"], dtype=torch.float32).to(\"mps\"),\n",
    "        return_tensors=\"pt\",\n",
    "        do_rescale=False,\n",
    "    ).pixel_values.squeeze(\n",
    "        0\n",
    "    )  # Remove batch dimension\n",
    "\n",
    "    label = torch.tensor(example[\"label\"], dtype=torch.long).to(\"mps\")\n",
    "\n",
    "    # Debugging\n",
    "    torch.mps.synchronize()  # Ensure all GPU operations are complete\n",
    "    print(\"Pixel values (type):\", type(pixel_values))\n",
    "    print(\n",
    "        \"Pixel values (shape):\", pixel_values.cpu().shape\n",
    "    )  # Move to CPU for inspection\n",
    "    print(\"Label (type):\", type(label))\n",
    "    print(\"Label (value):\", label.cpu().item())  # Move to CPU for inspection\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"label\": label,\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply preprocessing with memory efficiency\n",
    "train_dataset_small = train_dataset.select(range(100))\n",
    "test_dataset_small = test_dataset.select(range(100))\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.map(preprocess_data)\n",
    "# test_dataset = test_dataset.map(preprocess_data)\n",
    "\n",
    "train_dataset_small = train_dataset_small.map(preprocess_data)\n",
    "test_dataset_small = test_dataset_small.map(preprocess_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def data_collator(features: List[dict]) -> Dict[str, torch.Tensor]:\n",
    "    pixel_values = []\n",
    "    for idx, feature in enumerate(features):\n",
    "        if isinstance(feature[\"pixel_values\"], list):\n",
    "            print(f\"Feature {idx} pixel_values is a list. Converting to tensor.\")\n",
    "            pixel_values.append(\n",
    "                torch.tensor(feature[\"pixel_values\"], dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            pixel_values.append(feature[\"pixel_values\"])\n",
    "\n",
    "    # Stack pixel_values and squeeze the extra dimension if present\n",
    "    pixel_values = (\n",
    "        torch.stack(pixel_values).squeeze(1).to(\"mps\")\n",
    "    )  # Squeeze the singleton dimension\n",
    "\n",
    "    # Convert labels to tensors and move to the appropriate device\n",
    "    labels = torch.tensor([f[\"label\"] for f in features], dtype=torch.long).to(\"mps\")\n",
    "\n",
    "    # Debugging: Confirm the shapes\n",
    "    print(\"Pixel values shape after stacking and squeezing:\", pixel_values.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 pixel_values is a list. Converting to tensor.\n",
      "Feature 1 pixel_values is a list. Converting to tensor.\n",
      "Feature 2 pixel_values is a list. Converting to tensor.\n",
      "Feature 3 pixel_values is a list. Converting to tensor.\n",
      "Feature 4 pixel_values is a list. Converting to tensor.\n",
      "Feature 5 pixel_values is a list. Converting to tensor.\n",
      "Feature 6 pixel_values is a list. Converting to tensor.\n",
      "Feature 7 pixel_values is a list. Converting to tensor.\n",
      "Feature 8 pixel_values is a list. Converting to tensor.\n",
      "Feature 9 pixel_values is a list. Converting to tensor.\n",
      "Feature 10 pixel_values is a list. Converting to tensor.\n",
      "Feature 11 pixel_values is a list. Converting to tensor.\n",
      "Feature 12 pixel_values is a list. Converting to tensor.\n",
      "Feature 13 pixel_values is a list. Converting to tensor.\n",
      "Feature 14 pixel_values is a list. Converting to tensor.\n",
      "Feature 15 pixel_values is a list. Converting to tensor.\n",
      "Pixel values shape after stacking and squeezing: torch.Size([16, 3, 384, 384])\n",
      "Labels shape: torch.Size([16])\n",
      "Feature 0 pixel_values is a list. Converting to tensor.\n",
      "Feature 1 pixel_values is a list. Converting to tensor.\n",
      "Feature 2 pixel_values is a list. Converting to tensor.\n",
      "Feature 3 pixel_values is a list. Converting to tensor.\n",
      "Feature 4 pixel_values is a list. Converting to tensor.\n",
      "Feature 5 pixel_values is a list. Converting to tensor.\n",
      "Feature 6 pixel_values is a list. Converting to tensor.\n",
      "Feature 7 pixel_values is a list. Converting to tensor.\n",
      "Feature 8 pixel_values is a list. Converting to tensor.\n",
      "Feature 9 pixel_values is a list. Converting to tensor.\n",
      "Feature 10 pixel_values is a list. Converting to tensor.\n",
      "Feature 11 pixel_values is a list. Converting to tensor.\n",
      "Feature 12 pixel_values is a list. Converting to tensor.\n",
      "Feature 13 pixel_values is a list. Converting to tensor.\n",
      "Feature 14 pixel_values is a list. Converting to tensor.\n",
      "Feature 15 pixel_values is a list. Converting to tensor.\n",
      "Pixel values shape after stacking and squeezing: torch.Size([16, 3, 384, 384])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m   3675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs:\n\u001b[0;32m-> 3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/trainer.py:3734\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3732\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3733\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3734\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:615\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m encoder_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 615\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mshift_tokens_right\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_start_token_id\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m    620\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    621\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m    622\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_decoder,\n\u001b[1;32m    632\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:42\u001b[0m, in \u001b[0;36mshift_tokens_right\u001b[0;34m(input_ids, pad_token_id, decoder_start_token_id)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mShift input ids one token to the right.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m shifted_input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mnew_zeros(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 42\u001b[0m shifted_input_ids[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_start_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to set the decoder_start_token_id attribute of the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"src/model/trained_model/\")\n",
    "processor.save_pretrained(\"src/model/trained_model/\")\n",
    "print(f\"Model and processor saved to src/model/trained_model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
